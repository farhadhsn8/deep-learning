{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7DNimdo9GLBW"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install einops -q"
      ],
      "metadata": {
        "id": "b84gFzMdHMNa"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install wandb"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FpPdCc2pHZ7E",
        "outputId": "e48ca841-40d2-4332-d721-64453bbb65a8"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: wandb in /usr/local/lib/python3.8/dist-packages (0.13.7)\n",
            "Requirement already satisfied: docker-pycreds>=0.4.0 in /usr/local/lib/python3.8/dist-packages (from wandb) (0.4.0)\n",
            "Requirement already satisfied: sentry-sdk>=1.0.0 in /usr/local/lib/python3.8/dist-packages (from wandb) (1.9.0)\n",
            "Requirement already satisfied: GitPython>=1.0.0 in /usr/local/lib/python3.8/dist-packages (from wandb) (3.1.29)\n",
            "Requirement already satisfied: setproctitle in /usr/local/lib/python3.8/dist-packages (from wandb) (1.3.2)\n",
            "Requirement already satisfied: psutil>=5.0.0 in /usr/local/lib/python3.8/dist-packages (from wandb) (5.4.8)\n",
            "Requirement already satisfied: PyYAML in /usr/local/lib/python3.8/dist-packages (from wandb) (6.0)\n",
            "Requirement already satisfied: shortuuid>=0.5.0 in /usr/local/lib/python3.8/dist-packages (from wandb) (1.0.11)\n",
            "Requirement already satisfied: requests<3,>=2.0.0 in /usr/local/lib/python3.8/dist-packages (from wandb) (2.23.0)\n",
            "Requirement already satisfied: promise<3,>=2.0 in /usr/local/lib/python3.8/dist-packages (from wandb) (2.3)\n",
            "Requirement already satisfied: protobuf!=4.21.0,<5,>=3.12.0 in /usr/local/lib/python3.8/dist-packages (from wandb) (3.19.6)\n",
            "Requirement already satisfied: pathtools in /usr/local/lib/python3.8/dist-packages (from wandb) (0.1.2)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.8/dist-packages (from wandb) (57.4.0)\n",
            "Requirement already satisfied: Click!=8.0.0,>=7.0 in /usr/local/lib/python3.8/dist-packages (from wandb) (7.1.2)\n",
            "Requirement already satisfied: six>=1.4.0 in /usr/local/lib/python3.8/dist-packages (from docker-pycreds>=0.4.0->wandb) (1.15.0)\n",
            "Requirement already satisfied: gitdb<5,>=4.0.1 in /usr/local/lib/python3.8/dist-packages (from GitPython>=1.0.0->wandb) (4.0.10)\n",
            "Requirement already satisfied: smmap<6,>=3.0.1 in /usr/local/lib/python3.8/dist-packages (from gitdb<5,>=4.0.1->GitPython>=1.0.0->wandb) (5.0.0)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.8/dist-packages (from requests<3,>=2.0.0->wandb) (1.24.3)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.8/dist-packages (from requests<3,>=2.0.0->wandb) (2.10)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.8/dist-packages (from requests<3,>=2.0.0->wandb) (2022.9.24)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.8/dist-packages (from requests<3,>=2.0.0->wandb) (3.0.4)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from typing import Callable\n",
        "import time\n",
        "import random\n",
        "import math\n",
        "\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import wandb\n",
        "\n",
        "from PIL import Image\n",
        "from einops import rearrange, reduce, repeat\n",
        "from torch.utils.data import DataLoader\n",
        "from torchvision import transforms\n",
        "from torchvision.datasets import CIFAR10\n",
        "from torchvision.datasets import CIFAR100"
      ],
      "metadata": {
        "id": "TUQgmmtwHPzH"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class MSA(nn.Module):\n",
        "    \"\"\"Multi-head Self Attention Block\"\"\"\n",
        "\n",
        "    def __init__(\n",
        "        self, heads: int, emb_dim: int, \n",
        "        dropout: float = 0., attention_dropout: float = 0.\n",
        "    ):\n",
        "        super().__init__()\n",
        "        self.emb_dim = emb_dim\n",
        "        self.n_h = heads\n",
        "        self.head_dim = self.emb_dim // self.n_h\n",
        "        self.q = nn.Linear(self.emb_dim, self.emb_dim)\n",
        "        self.k = nn.Linear(self.emb_dim, self.emb_dim)\n",
        "        self.v = nn.Linear(self.emb_dim, self.emb_dim)\n",
        "        self.attention_dropout = nn.Dropout(attention_dropout)\n",
        "        self.linear_projection = nn.Linear(self.emb_dim, self.emb_dim)\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "    def forward(self, x: torch.Tensor):\n",
        "        # (bs,     s_l,      e_d)\n",
        "        batch_s, seq_len, emb_dim = x.shape\n",
        "        # (bs, s_l, e_d) -> (bs, s_l, n_h, h_d) -> (bs, n_h, s_l, h_d)\n",
        "        x_q = self.q(x).view(\n",
        "            batch_s, seq_len, self.n_h, self.head_dim).transpose(1, 2)\n",
        "        x_k = self.k(x).view(\n",
        "            batch_s, seq_len, self.n_h, self.head_dim).transpose(1, 2)\n",
        "        x_v = self.v(x).view(\n",
        "            batch_s, seq_len, self.n_h, self.head_dim).transpose(1, 2)\n",
        "        # @ operator is the convention for matrix multiplication, throughout python\n",
        "        # q @ k.T -> (bs, n_h, s_l, h_d) @ (bs, n_h, h_d, s_l) -> (bs, n_h, s_l, s_l)\n",
        "        # Softmax((q @ k.T)/root(h_d)) @ v\n",
        "        #   -> (bs, n_h, s_l, s_l) @ (bs, n_h, s_l, h_d) -> (bs, n_h, s_l, h_d)\n",
        "        attention = (x_q @ x_k.transpose(-2, -1)) / math.sqrt(x_q.size(-1))\n",
        "        attention = F.softmax(attention, dim=-1)\n",
        "        attention = self.attention_dropout(attention)\n",
        "        # (bs, n_h, s_l, h_d) -> (bs, s_l, n_h, h_d) -> (bs, s_l, e_d)\n",
        "        x = (attention @ x_v).transpose(1, 2).reshape(batch_s, seq_len, emb_dim)\n",
        "        x = self.linear_projection(x)\n",
        "        x = self.dropout(x)\n",
        "        return x"
      ],
      "metadata": {
        "id": "MdG2VBzRHUMr"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class MLP(nn.Module):\n",
        "    \"\"\"MLP block\"\"\"\n",
        "\n",
        "    def __init__(self, emb_dim: int, feat_dim: int, dropout: float = 0):\n",
        "        super().__init__()\n",
        "        self.layer1 = nn.Linear(emb_dim, feat_dim)\n",
        "        self.activation = nn.GELU()\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "        self.layer2 = nn.Linear(feat_dim, emb_dim)\n",
        "\n",
        "        # below init from torchvision\n",
        "        nn.init.xavier_uniform_(self.layer1.weight)\n",
        "        nn.init.xavier_uniform_(self.layer2.weight)\n",
        "        nn.init.normal_(self.layer1.bias, std=1e-6)\n",
        "        nn.init.normal_(self.layer2.bias, std=1e-6)\n",
        "\n",
        "    def forward(self, x: torch.Tensor):\n",
        "        x = self.layer1(x)\n",
        "        x = self.activation(x)\n",
        "        x = self.dropout(x)\n",
        "        x = self.layer2(x)\n",
        "        x = self.dropout(x)\n",
        "        return x"
      ],
      "metadata": {
        "id": "wNXw_OB8Ho9c"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class TransformerEncoderBlock(nn.Module):\n",
        "    \"\"\"Transformer Encoder Block\"\"\"\n",
        "\n",
        "    def __init__(\n",
        "        self, n_h: int, emb_dim: int, feat_dim: int, \n",
        "        dropout: float = 0, attention_dropout: float = 0\n",
        "    ):\n",
        "        super().__init__()\n",
        "        self.msa = MSA(heads=n_h, emb_dim=emb_dim, dropout=dropout, attention_dropout=attention_dropout)\n",
        "        self.norm1 = nn.LayerNorm(emb_dim)\n",
        "        self.ffn = MLP(emb_dim, feat_dim, dropout)\n",
        "        self.norm2 = nn.LayerNorm(emb_dim)\n",
        "\n",
        "    def forward(self, x: torch.Tensor):\n",
        "        identity = x\n",
        "        x = self.msa(x)\n",
        "        x += identity\n",
        "        x = self.norm1(x)\n",
        "        identity = x\n",
        "        x = self.ffn(x)\n",
        "        x += identity\n",
        "        x = self.norm2(x)\n",
        "        return x"
      ],
      "metadata": {
        "id": "XFKF8FOMHr4W"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class ViT(nn.Module):\n",
        "    \"\"\"\n",
        "        Vison Transformer (ViT) Model\n",
        "        https://arxiv.org/abs/2010.11929\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(\n",
        "        self, heads: int = 12, emb_dim: int = 768, feat_dim: int = 3072, \n",
        "        dropout: float = 0, layers: int = 12, patch_size: int = 16, \n",
        "        channels: int = 3, image_size: int = 224, num_class: int = 1000\n",
        "    ):\n",
        "        super().__init__()\n",
        "        self.patch_size = patch_size\n",
        "        self.image_size = image_size\n",
        "        self.linear_projection = nn.Linear(\n",
        "            self.patch_size*self.patch_size*channels, emb_dim)\n",
        "        self.class_token = nn.Parameter(torch.randn([1, 1, emb_dim]))\n",
        "        self.pos_emb = nn.Parameter(\n",
        "            torch.randn(\n",
        "                [1, self.image_size // patch_size * self.image_size // patch_size + 1, emb_dim]\n",
        "            ).normal_(std=0.02) # init from torchvision, which is inspired by BERT\n",
        "        )\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "        encoders = []\n",
        "        for _ in range(0, layers):\n",
        "            encoders.append(\n",
        "                TransformerEncoderBlock(\n",
        "                    n_h=heads, emb_dim=emb_dim, feat_dim=feat_dim,\n",
        "                    dropout=dropout\n",
        "                )\n",
        "            )\n",
        "        self.encoder_stack = nn.Sequential(*encoders)\n",
        "        self.mlp_head = nn.Linear(emb_dim, num_class)\n",
        "\n",
        "    def forward(self, x: torch.Tensor):\n",
        "        bs, c, h, w = x.shape  # (bs, c, h, w)\n",
        "        torch._assert(h*w == self.image_size**2, \"image size mismatch\")\n",
        "        torch._assert(\n",
        "            (self.image_size ** 2) % (self.patch_size ** 2) == 0, \n",
        "            \"image not disivible by patch_size\"\n",
        "        )\n",
        "\n",
        "        # below 2 lines creates patches using view/reshape and permutations\n",
        "        # (bs, c, no_of_patches, patch_h, no_of_patches, patch_w) \n",
        "        #   -> (bs, no_of_patches, no_of_patches, c, patch_h, patch_w)\n",
        "        x = x.view(\n",
        "            bs, c, h // self.patch_size, self.patch_size, \n",
        "            w // self.patch_size, self.patch_size\n",
        "        ).permute(0, 2, 4, 1, 3, 5)\n",
        "        # (bs, no_of_patches*no_of_patches, c*patch_h*patch_w)\n",
        "        x = x.reshape(\n",
        "            bs, h // self.patch_size * w // self.patch_size, \n",
        "            c*self.patch_size*self.patch_size\n",
        "        )\n",
        "\n",
        "        # linear projection to embedding dimension\n",
        "        x = self.linear_projection(x)\n",
        "\n",
        "        # this expands/repeats the [class] token for each batch. Expand uses less memory\n",
        "        # alternative: self.batch_class_token.repeat((bs, 1, 1), 1)\n",
        "        batch_class_token = self.class_token.expand(bs, -1, -1)\n",
        "\n",
        "        # appends sequence to the [class] token (dimension 1) \n",
        "        # (bs, seq_len, emb_dim) -> (bs, seq_len + 1, emb_dim)\n",
        "        x = torch.cat((batch_class_token, x), 1)\n",
        "\n",
        "        # expands position embedding batch wise, add embedding dropout\n",
        "        x = self.pos_emb.expand(bs, -1, -1) + x\n",
        "        x = self.dropout(x)\n",
        "\n",
        "        # sequentially pass through all encoders\n",
        "        x = self.encoder_stack(x)\n",
        "\n",
        "        # collect output from [class] token\n",
        "        x = x[:, 0]\n",
        "\n",
        "        # send it through MLP head to get logits\n",
        "        x = self.mlp_head(x)\n",
        "\n",
        "        return x"
      ],
      "metadata": {
        "id": "UBUet6K9Huh-"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class ConvTokenizer(nn.Module):\n",
        "    def __init__(\n",
        "        self,\n",
        "        channels: int = 3, emb_dim: int = 256,\n",
        "        conv_kernel: int = 3, conv_stride: int = 2, conv_pad: int = 3,\n",
        "        pool_kernel: int = 3, pool_stride: int = 2, pool_pad: int = 1,\n",
        "        activation: Callable = nn.ReLU\n",
        "    ):\n",
        "        super().__init__()\n",
        "        self.conv = nn.Conv2d(\n",
        "            in_channels=channels, out_channels=emb_dim,\n",
        "            kernel_size=conv_kernel, stride=conv_stride,\n",
        "            padding=(conv_pad, conv_pad)\n",
        "        )\n",
        "        self.act = activation(inplace=True)\n",
        "        self.max_pool = nn.MaxPool2d(\n",
        "            kernel_size=pool_kernel, stride=pool_stride, \n",
        "            padding=pool_pad\n",
        "        )\n",
        "            \n",
        "    def forward(self, x: torch.Tensor):\n",
        "        x = self.conv(x)\n",
        "        x = self.act(x)\n",
        "        x = self.max_pool(x)\n",
        "        return x"
      ],
      "metadata": {
        "id": "Ho7XrMvSH0lX"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "x = torch.randn([1, 3, 32, 32])\n",
        "t = ConvTokenizer()\n",
        "t(x).shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "m-jLPEtlH4m_",
        "outputId": "b926de54-d3a0-4919-839b-526d6d4704ad"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([1, 256, 9, 9])"
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class SeqPool(nn.Module):\n",
        "    def __init__(self, emb_dim=256):\n",
        "        super().__init__()\n",
        "        self.dense = nn.Linear(emb_dim, 1)\n",
        "        self.softmax = nn.Softmax(dim=-1)\n",
        "\n",
        "    def forward(self, x):\n",
        "        bs, seq_len, emb_dim = x.shape\n",
        "        identity = x\n",
        "        x = self.dense(x)\n",
        "        x = rearrange(\n",
        "            x, 'bs seq_len 1 -> bs 1 seq_len', seq_len=seq_len\n",
        "        )\n",
        "        x = self.softmax(x)\n",
        "        x = x @ identity\n",
        "        x = rearrange(\n",
        "            x, 'bs 1 e_d -> bs e_d', e_d=emb_dim\n",
        "        )\n",
        "        return x"
      ],
      "metadata": {
        "id": "zFKgt7p4H7NC"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "x = torch.randn([8, 36, 256])\n",
        "p = SeqPool()\n",
        "p(x).shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zYNwrdL7H-Df",
        "outputId": "3a8b688d-0876-4f0e-9607-bd010746dad1"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([8, 256])"
            ]
          },
          "metadata": {},
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class CCT(nn.Module):\n",
        "    \"\"\"\n",
        "        Compact Convolutional Transformer (CCT) Model\n",
        "        https://arxiv.org/abs/2104.05704v4\n",
        "    \"\"\"    \n",
        "    def __init__(\n",
        "        self,\n",
        "        conv_kernel: int = 3, conv_stride: int = 2, conv_pad: int = 3,\n",
        "        pool_kernel: int = 3, pool_stride: int = 2, pool_pad: int = 1,\n",
        "        heads: int = 4, emb_dim: int = 256, feat_dim: int = 2*256, \n",
        "        dropout: float = 0.1, attention_dropout: float = 0.1, layers: int = 7, \n",
        "        channels: int = 3, image_size: int = 32, num_class: int = 10\n",
        "    ):\n",
        "        super().__init__()\n",
        "        self.emb_dim = emb_dim\n",
        "        self.image_size = image_size\n",
        "\n",
        "        self.tokenizer = ConvTokenizer(\n",
        "            channels=channels, emb_dim=self.emb_dim,\n",
        "            conv_kernel=conv_kernel, conv_stride=conv_stride, conv_pad=conv_pad,\n",
        "            pool_kernel=pool_kernel, pool_stride=pool_stride, pool_pad=pool_pad,\n",
        "            activation=nn.ReLU\n",
        "        )\n",
        "\n",
        "        with torch.no_grad():\n",
        "            x = torch.randn([1, channels, image_size, image_size])\n",
        "            out = self.tokenizer(x)\n",
        "            _, _, ph_c, pw_c  = out.shape\n",
        "\n",
        "        self.linear_projection = nn.Linear(\n",
        "            ph_c, pw_c, self.emb_dim\n",
        "        )\n",
        "\n",
        "        self.pos_emb = nn.Parameter(\n",
        "            torch.randn(\n",
        "                [1, ph_c*pw_c, self.emb_dim]\n",
        "            ).normal_(std=0.02) # from torchvision, which takes this from BERT\n",
        "        )\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "        encoders = []\n",
        "        for _ in range(0, layers):\n",
        "            encoders.append(\n",
        "                TransformerEncoderBlock(\n",
        "                    n_h=heads, emb_dim=self.emb_dim, feat_dim=feat_dim,\n",
        "                    dropout=dropout, attention_dropout=attention_dropout\n",
        "                )\n",
        "            )\n",
        "        self.encoder_stack = nn.Sequential(*encoders)\n",
        "        self.seq_pool = SeqPool(emb_dim=self.emb_dim)\n",
        "        self.mlp_head = nn.Linear(self.emb_dim, num_class)\n",
        "\n",
        "\n",
        "    def forward(self, x: torch.Tensor):     \n",
        "        bs, c, h, w = x.shape  # (bs, c, h, w)\n",
        "\n",
        "        # Creates overlapping patches using ConvNet\n",
        "        x = self.tokenizer(x)\n",
        "        x = rearrange(\n",
        "            x, 'bs e_d ph_h ph_w -> bs (ph_h ph_w) e_d', \n",
        "            bs=bs, e_d=self.emb_dim\n",
        "        )\n",
        "\n",
        "        # Add position embedding\n",
        "        x = self.pos_emb.expand(bs, -1, -1) + x\n",
        "        x = self.dropout(x)\n",
        "\n",
        "        # Pass through Transformer Encoder layers\n",
        "        x = self.encoder_stack(x)\n",
        "\n",
        "        # Perform Sequential Pooling <- Novelty of the paper\n",
        "        x = self.seq_pool(x)\n",
        "\n",
        "        # MLP head used to get logits\n",
        "        x = self.mlp_head(x)\n",
        "\n",
        "        return x"
      ],
      "metadata": {
        "id": "YUgOXMPCIArv"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "x = torch.randn([8, 3, 32, 32])\n",
        "t = CCT()\n",
        "t(x).shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qAp2RpJNIK_A",
        "outputId": "1c914902-c41e-40d9-d853-e12d2ca9eba4"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([8, 10])"
            ]
          },
          "metadata": {},
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install ptflops -q "
      ],
      "metadata": {
        "id": "38sG6E_GIPLJ"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from ptflops import get_model_complexity_info\n",
        "\n",
        "with torch.cuda.device(0):\n",
        "  macs, params = get_model_complexity_info(t, (3, 32, 32), as_strings=True, print_per_layer_stat=False)\n",
        "  print('{:<30}  {:<8}'.format('Computational complexity: ', macs))\n",
        "  print('{:<30}  {:<8}'.format('Number of parameters: ', params))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OkgYUl96IT3l",
        "outputId": "25a94532-e326-46fe-bd0b-cd479b65f407"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Computational complexity:       300.09 MMac\n",
            "Number of parameters:           3.72 M  \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "mean = (0.4914, 0.4822, 0.4465)\n",
        "std = (0.2470, 0.2435, 0.2616)\n",
        "\n",
        "def get_train_transforms(image_size=(32, 32)):\n",
        "    return transforms.Compose(\n",
        "        [   \n",
        "            transforms.RandAugment(),\n",
        "            transforms.RandomResizedCrop(\n",
        "                image_size, scale=(0.5, 1.5)\n",
        "            ),\n",
        "            transforms.RandomHorizontalFlip(),\n",
        "            transforms.ToTensor(),\n",
        "            transforms.Normalize(mean, std, True)\n",
        "        ]\n",
        "    )\n",
        "\n",
        "def get_val_transforms(image_size=(32, 32)):\n",
        "    return transforms.Compose(\n",
        "        [\n",
        "            transforms.ToTensor(),\n",
        "            transforms.Normalize(mean, std, True)\n",
        "        ]\n",
        "    )"
      ],
      "metadata": {
        "id": "g8C4lJu4IWLE"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def seed_everything(seed):\n",
        "    random.seed(seed)\n",
        "    np.random.seed(seed)\n",
        "    torch.manual_seed(seed)\n",
        "    torch.cuda.manual_seed(seed)\n",
        "    torch.backends.cudnn.deterministic = True\n",
        "    torch.backends.cudnn.benchmark = False\n",
        "\n",
        "# below func from torchvision\n",
        "def accuracy(output, target, topk=(1,)):\n",
        "    \"\"\"\n",
        "    Computes the accuracy over the k top predictions \n",
        "    for the specified values of k\n",
        "    \"\"\"\n",
        "    with torch.inference_mode():\n",
        "        maxk = max(topk)\n",
        "        batch_size = target.size(0)\n",
        "        if target.ndim == 2:\n",
        "            target = target.max(dim=1)[1]\n",
        "\n",
        "        _, pred = output.topk(maxk, 1, True, True)\n",
        "        pred = pred.t()\n",
        "        correct = pred.eq(target[None])\n",
        "\n",
        "        res = []\n",
        "        for k in topk:\n",
        "            correct_k = correct[:k].flatten().sum(dtype=torch.float32)\n",
        "            res.append(correct_k * (100.0 / batch_size))\n",
        "        return res\n",
        "\n",
        "\n",
        "def train_one_epoch(\n",
        "    model, criterion, optimizer, scheduler, data_loader, device\n",
        "):\n",
        "    model.train()\n",
        "    running_loss = 0\n",
        "    start_time = time.time()\n",
        "    for image, target in data_loader:\n",
        "        image, target = image.to(device), target.to(device)\n",
        "        output = model(image)\n",
        "        loss = criterion(output, target)\n",
        "        running_loss += float(loss.item())\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        if scheduler is not None:\n",
        "            scheduler.step()\n",
        "    print(f'Elapsed time {(time.time() - start_time):.1f}s')\n",
        "    return running_loss/len(data_loader)\n",
        "\n",
        "\n",
        "@torch.inference_mode()\n",
        "def val_one_epoch(model, criterion, data_loader, device):\n",
        "    model.eval()\n",
        "    acc1_list, acc5_list = [], []\n",
        "    running_loss = 0\n",
        "    with torch.inference_mode():\n",
        "        for image, target in data_loader:\n",
        "            image = image.to(device)\n",
        "            target = target.to(device)\n",
        "            output = model(image)\n",
        "            loss = criterion(output, target)\n",
        "            running_loss += float(loss.item())\n",
        "            acc1, acc5 = accuracy(output, target, topk=(1, 5))\n",
        "            acc1_list.append(acc1.detach().cpu().numpy())\n",
        "            acc5_list.append(acc5.detach().cpu().numpy())\n",
        "\n",
        "    return (\n",
        "        running_loss/len(data_loader), \n",
        "        np.mean(np.array(acc1_list)), \n",
        "        np.mean(np.array(acc5_list))\n",
        "    )"
      ],
      "metadata": {
        "id": "tH0q7KOqIaRf"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def train(model, config , train_ds , val_ds  , log=True):\n",
        "   \n",
        "   \n",
        "    train_dl = DataLoader(\n",
        "        train_ds, batch_size=config['batch_size'], shuffle=True, \n",
        "        pin_memory=True, num_workers=config['num_workers'], \n",
        "        persistent_workers=True\n",
        "    )\n",
        "\n",
        "    val_dl = DataLoader(\n",
        "        val_ds, batch_size=config['batch_size'], shuffle=False, \n",
        "        pin_memory=True, num_workers=config['num_workers']\n",
        "    )\n",
        "    criterion = nn.CrossEntropyLoss(label_smoothing=0.1)\n",
        "    device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "    model.to(device)\n",
        "    optimizer = torch.optim.AdamW(\n",
        "        model.parameters(), lr=config['lr'], weight_decay=config['weight_decay']\n",
        "    )\n",
        "    \n",
        "    # the authors used cosine annealing scheduler.\n",
        "    scheduler = torch.optim.lr_scheduler.OneCycleLR(\n",
        "        optimizer, max_lr=config['lr'], \n",
        "        steps_per_epoch=len(train_dl), epochs=config['max_epoch']\n",
        "    )\n",
        "    print('-'*50)\n",
        "    last_acc1 = 0.0\n",
        "    for i in range(0, config['max_epoch']):\n",
        "        loss = train_one_epoch(\n",
        "            model, criterion, optimizer, scheduler, train_dl, device\n",
        "        )\n",
        "        print(f'average train loss for epoch #{i+1} is {loss:.4f}')\n",
        "        loss, acc1, acc5 = val_one_epoch(model, criterion, val_dl, device)\n",
        "       \n",
        "        print(f'average val loss for epoch #{i+1} is {loss:.4f}')\n",
        "        print(f'average val acc@1 for epoch #{i+1} is {acc1:.4f}')\n",
        "        print(f'average val acc@5 for epoch #{i+1} is {acc5:.4f}')\n",
        "        if acc1 > last_acc1:\n",
        "            last_acc1 = acc1\n",
        "            torch.save(\n",
        "                model.state_dict(), \n",
        "                f'./cct_epoch_{i+1}_acc_{acc1:.2f}.pth'\n",
        "            )\n",
        "        print('-'*50)\n",
        "    \n"
      ],
      "metadata": {
        "id": "p_1gUJSxIfg-"
      },
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## on cifar10"
      ],
      "metadata": {
        "id": "LfUaHBuvLNlo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "config = dict(\n",
        "    seed=42, # <-- the answer to life, the universe and everything\n",
        "    heads=4, emb_dim=256, feat_dim=512, layers=7,\n",
        "    num_class=10, image_size=32,\n",
        "    dropout=0.1, attention_dropout=0.1,\n",
        "    batch_size=128, max_epoch=5,\n",
        "    lr=5e-4, weight_decay=3e-2,\n",
        "    num_workers=2\n",
        ")\n",
        "seed_everything(config['seed'])\n",
        "wandb.config = config\n",
        "model = CCT(\n",
        "    heads=config['heads'], emb_dim=config['emb_dim'], \n",
        "    feat_dim=config['feat_dim'], layers=config['layers'],\n",
        "    num_class=config['num_class'], image_size=config['image_size'], \n",
        "    dropout=config['dropout'], attention_dropout=config['attention_dropout']\n",
        ")\n",
        "\n",
        "\n",
        "train_ds = CIFAR10(\n",
        "        root='/content/data/',\n",
        "        train=True,\n",
        "        transform=get_train_transforms(),\n",
        "        download=True\n",
        "    ) \n",
        " \n",
        "\n",
        "val_ds = CIFAR10(\n",
        "        root='/content/data/',\n",
        "        train=False,\n",
        "        transform=get_val_transforms(),\n",
        "        download=True\n",
        "    )\n",
        "\n",
        "train(model, config , train_ds , val_ds  , log=False)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZldHBDNoIlUz",
        "outputId": "6e9f350e-1441-4f6d-ceaa-8ed0379332f7"
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Files already downloaded and verified\n",
            "Files already downloaded and verified\n",
            "--------------------------------------------------\n",
            "Elapsed time 54.3s\n",
            "average train loss for epoch #1 is 2.0434\n",
            "average val loss for epoch #1 is 1.7846\n",
            "average val acc@1 for epoch #1 is 40.1800\n",
            "average val acc@5 for epoch #1 is 88.9339\n",
            "--------------------------------------------------\n",
            "Elapsed time 51.9s\n",
            "average train loss for epoch #2 is 1.7707\n",
            "average val loss for epoch #2 is 1.5726\n",
            "average val acc@1 for epoch #2 is 49.8220\n",
            "average val acc@5 for epoch #2 is 94.5016\n",
            "--------------------------------------------------\n",
            "Elapsed time 48.9s\n",
            "average train loss for epoch #3 is 1.6028\n",
            "average val loss for epoch #3 is 1.3963\n",
            "average val acc@1 for epoch #3 is 59.0684\n",
            "average val acc@5 for epoch #3 is 95.9553\n",
            "--------------------------------------------------\n",
            "Elapsed time 51.4s\n",
            "average train loss for epoch #4 is 1.4683\n",
            "average val loss for epoch #4 is 1.2970\n",
            "average val acc@1 for epoch #4 is 64.1614\n",
            "average val acc@5 for epoch #4 is 97.0332\n",
            "--------------------------------------------------\n",
            "Elapsed time 49.1s\n",
            "average train loss for epoch #5 is 1.3788\n",
            "average val loss for epoch #5 is 1.2470\n",
            "average val acc@1 for epoch #5 is 66.5150\n",
            "average val acc@5 for epoch #5 is 97.2409\n",
            "--------------------------------------------------\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## on cifar100"
      ],
      "metadata": {
        "id": "ZpRqBdS2NkDd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "config = dict(\n",
        "    seed=42, # <-- the answer to life, the universe and everything\n",
        "    heads=4, emb_dim=256, feat_dim=512, layers=7,\n",
        "    num_class=100, image_size=32,\n",
        "    dropout=0.1, attention_dropout=0.1,\n",
        "    batch_size=128, max_epoch=5,\n",
        "    lr=5e-4, weight_decay=3e-2,\n",
        "    num_workers=2\n",
        ")\n",
        "seed_everything(config['seed'])\n",
        "wandb.config = config\n",
        "model = CCT(\n",
        "    heads=config['heads'], emb_dim=config['emb_dim'], \n",
        "    feat_dim=config['feat_dim'], layers=config['layers'],\n",
        "    num_class=config['num_class'], image_size=config['image_size'], \n",
        "    dropout=config['dropout'], attention_dropout=config['attention_dropout']\n",
        ")\n",
        "\n",
        "\n",
        "train_ds = CIFAR100(\n",
        "        root='/content/data/',\n",
        "        train=True,\n",
        "        transform=get_train_transforms(),\n",
        "        download=True\n",
        "    ) \n",
        " \n",
        "\n",
        "val_ds = CIFAR100(\n",
        "        root='/content/data/',\n",
        "        train=False,\n",
        "        transform=get_val_transforms(),\n",
        "        download=True\n",
        "    )\n",
        "\n",
        "train(model, config , train_ds , val_ds  , log=False)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Yfsd0xF3IphS",
        "outputId": "798da66f-8ef8-4cf5-da39-1e877a049dba"
      },
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Files already downloaded and verified\n",
            "Files already downloaded and verified\n",
            "--------------------------------------------------\n",
            "Elapsed time 48.9s\n",
            "average train loss for epoch #1 is 4.3304\n",
            "average val loss for epoch #1 is 3.9395\n",
            "average val acc@1 for epoch #1 is 11.2737\n",
            "average val acc@5 for epoch #1 is 33.8113\n",
            "--------------------------------------------------\n",
            "Elapsed time 49.0s\n",
            "average train loss for epoch #2 is 3.9078\n",
            "average val loss for epoch #2 is 3.5983\n",
            "average val acc@1 for epoch #2 is 17.8600\n",
            "average val acc@5 for epoch #2 is 45.1740\n",
            "--------------------------------------------------\n",
            "Elapsed time 50.6s\n",
            "average train loss for epoch #3 is 3.5982\n",
            "average val loss for epoch #3 is 3.2369\n",
            "average val acc@1 for epoch #3 is 26.1076\n",
            "average val acc@5 for epoch #3 is 57.7334\n",
            "--------------------------------------------------\n",
            "Elapsed time 49.0s\n",
            "average train loss for epoch #4 is 3.3372\n",
            "average val loss for epoch #4 is 3.0485\n",
            "average val acc@1 for epoch #4 is 31.6357\n",
            "average val acc@5 for epoch #4 is 62.6286\n",
            "--------------------------------------------------\n",
            "Elapsed time 48.5s\n",
            "average train loss for epoch #5 is 3.1584\n",
            "average val loss for epoch #5 is 2.9212\n",
            "average val acc@1 for epoch #5 is 34.3453\n",
            "average val acc@5 for epoch #5 is 66.4953\n",
            "--------------------------------------------------\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "PaxdCeiFLIZ2"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}